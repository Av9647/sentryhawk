#!/usr/bin/env python3
import os
import sys
import time
import json
import logging

import boto3
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from pyiceberg.catalog import load_catalog
from pyiceberg.manifest import ManifestEntryStatus

# Config from env
MODE              = os.getenv("MODE", "incremental")   # "backfill" or "incremental"
AWS_REGION        = os.getenv("AWS_REGION", "us-east-2")
DRUID_BASE        = os.getenv("DRUID_BASE", "http://10.0.0.21:8081")
DYNAMO_TABLE_NAME = os.getenv("DYNAMO_TABLE_NAME", "druid_ingest_meta")
WAREHOUSE         = os.getenv("WAREHOUSE", "s3://cve-production/")
ICEBERG_DB        = os.getenv("ICEBERG_DB", "cve_db")
BACKFILL_DIR      = os.getenv("BACKFILL_SPECS_PATH", "/app/backfill")
INCR_DIR          = os.getenv("INCR_SPECS_PATH",     "/app/incremental")

SUFFIXES = [
    "lookup",
    "daily_global",
    "daily_vendor",
    "daily_product",
    "monthly_global",
    "monthly_vendor",
    "monthly_product",
    "ytd_global",
    "ytd_vendor",
    "ytd_product",
    "daily_global_running",
    "daily_vendor_running",
    "daily_product_running"
]
TABLE_PREFIX = "cve_production_"

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger()

# HTTP session w/ retries & timeouts
session = requests.Session()
retries = Retry(
    total=3,
    backoff_factor=1,
    status_forcelist=[502,503,504],
    allowed_methods=["GET","POST"]
)
session.mount("http://", HTTPAdapter(max_retries=retries))

def submit_druid(spec: dict):
    """Submit an ingestion spec to Druid Coordinator and poll every minute."""
    url = f"{DRUID_BASE}/druid/indexer/v1/task"
    logger.debug("Submitting spec:\n%s", json.dumps(spec, indent=2))

    # fire off the task
    resp = session.post(
        url,
        json=spec,
        headers={"Content-Type":"application/json"},
        timeout=(5, 300)
    )
    if not resp.ok:
        logger.error("Druid POST failed [%d]: %s", resp.status_code, resp.text)
        resp.raise_for_status()

    task_id = resp.json()["task"]
    logger.info("Submitted Druid task %s", task_id)

    start = time.time()
    while True:
        try:
            st = session.get(
                f"{DRUID_BASE}/druid/indexer/v1/task/{task_id}/status",
                timeout=(5, 60)    # 5s connect, 60s read
            )
            st.raise_for_status()
            status = st.json()["status"]["status"]
        except Exception as e:
            # catch ConnectTimeout, ReadTimeout, HTTPError, etc.
            logger.warning("Status check error (%s); retrying in 60s", e)
            time.sleep(60)
            continue

        if status == "SUCCESS":
            elapsed = int(time.time() - start)
            logger.info("✔ Task %s succeeded in %ds", task_id, elapsed)
            return

        if status in ("FAILED", "ABORTED"):
            logger.error("✖ Task %s ended with status %s: %s", task_id, status, st.text)
            raise RuntimeError(f"Task {task_id} failed with status {status}")

        # still running
        logger.info("Task %s is %s; next check in 60s", task_id, status)
        time.sleep(60)

def bootstrap_dynamo(table_name: str):
    client = boto3.client("dynamodb", region_name=AWS_REGION)
    existing = client.list_tables()["TableNames"]
    if table_name not in existing:
        logger.info("Creating DynamoDB table %s", table_name)
        client.create_table(
            TableName=table_name,
            KeySchema=[{"AttributeName":"datasource","KeyType":"HASH"}],
            AttributeDefinitions=[{"AttributeName":"datasource","AttributeType":"S"}],
            BillingMode="PAY_PER_REQUEST"
        )
        client.get_waiter("table_exists").wait(TableName=table_name)
        logger.info("DynamoDB table %s ready", table_name)
    return boto3.resource("dynamodb", region_name=AWS_REGION).Table(table_name)

def main():
    catalog    = load_catalog(name="glue", type="glue", warehouse=WAREHOUSE)
    meta_table = bootstrap_dynamo(DYNAMO_TABLE_NAME)

    for suffix in SUFFIXES:
        tbl_name     = TABLE_PREFIX + suffix
        full_tbl     = f"{ICEBERG_DB}.{tbl_name}"
        datasource   = tbl_name
        logger.info("▶ Processing %s (MODE=%s)", full_tbl, MODE)

        # load table & snapshot
        table         = catalog.load_table(full_tbl)
        curr_snapshot = table.current_snapshot().snapshot_id

        # inspect manifest entries
        arrow_table   = table.inspect.entries()
        data_files    = arrow_table["data_file"].to_pylist()
        snapshot_ids  = arrow_table["snapshot_id"].to_pylist()
        statuses      = arrow_table["status"].to_pylist()
        file_paths    = [df["file_path"] for df in data_files]

        # backfill
        if MODE.lower() == "backfill":
            spec_file = os.path.join(BACKFILL_DIR, f"backfill_{suffix}.json")
            with open(spec_file) as f:
                spec = json.load(f)

            input_src = spec["spec"]["ioConfig"]["inputSource"]
            # remove mutually-exclusive keys
            input_src.pop("prefixes", None)
            input_src.pop("objects", None)
            # set exactly one: uris
            input_src["uris"] = file_paths

            spec["spec"]["dataSchema"]["dataSource"] = datasource
            submit_druid(spec)
            meta_table.put_item(
                Item={"datasource": datasource, "last_snapshot": curr_snapshot}
            )
            continue

        # incremental
        rec = meta_table.get_item(Key={"datasource": datasource}).get("Item", {})
        last_snapshot = rec.get("last_snapshot")
        if last_snapshot is None:
            raise RuntimeError(f"No watermark for {datasource}; run backfill first")

        uris = [
            fp
            for fp, sid, st in zip(file_paths, snapshot_ids, statuses)
            if st == ManifestEntryStatus.ADDED and sid > last_snapshot
        ]

        if not uris:
            logger.info("No new files; bumping watermark only")
            meta_table.update_item(
                Key={"datasource": datasource},
                UpdateExpression="SET last_snapshot = :s",
                ExpressionAttributeValues={":s": curr_snapshot},
            )
            continue

        spec_file = os.path.join(INCR_DIR, f"incremental_{suffix}.json")
        with open(spec_file) as f:
            spec = json.load(f)

        input_src = spec["spec"]["ioConfig"]["inputSource"]
        input_src.pop("prefixes", None)
        input_src.pop("objects", None)
        input_src.pop("objectGlob", None)
        input_src["uris"] = uris

        spec["spec"]["dataSchema"]["dataSource"] = datasource
        submit_druid(spec)
        meta_table.update_item(
            Key={"datasource": datasource},
            UpdateExpression="SET last_snapshot = :s",
            ExpressionAttributeValues={":s": curr_snapshot},
        )

    logger.info("All tables processed successfully")

if __name__ == "__main__":
    try:
        main()
    except Exception:
        logger.exception("Fatal error during ingestion")
        sys.exit(1)
