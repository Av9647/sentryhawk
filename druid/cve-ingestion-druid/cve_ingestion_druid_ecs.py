#!/usr/bin/env python3
import os
import sys
import time
import gzip
import json
import logging
from datetime import datetime, timezone

import boto3
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from pyiceberg.catalog import load_catalog
from pyiceberg.manifest import ManifestEntryStatus

# Config from env
MODE              = os.getenv("MODE", "backfill")   # "backfill" or "incremental"
AWS_REGION        = os.getenv("AWS_REGION", "us-east-2")
DRUID_BASE        = os.getenv("DRUID_BASE", "http://10.0.0.21:8081")
DYNAMO_TABLE_NAME = os.getenv("DYNAMO_TABLE_NAME", "druid_ingest_meta")
WAREHOUSE         = os.getenv("WAREHOUSE", "s3://cve-production/")
ICEBERG_DB        = os.getenv("ICEBERG_DB", "cve_db")
BACKFILL_DIR      = os.getenv("BACKFILL_SPECS_PATH", "/app/backfill")
INCR_DIR          = os.getenv("INCR_SPECS_PATH",     "/app/incremental")

SUFFIXES = [
    # "lookup",
    # "daily_global",
    # "daily_vendor",
    # "daily_product",
    # "monthly_global",
    # "monthly_vendor",
    # "monthly_product",
    # "yearly_global",
    # "yearly_vendor",
    # "yearly_product",
    # "daily_global_running",
    # "daily_vendor_running",
    # "daily_product_running",
    "trailing_12mo_vendor",
    "trailing_12mo_product",
    "trailing_1mo_vendor",
    "trailing_1mo_product",
    "exposure_index_thresholds"
]
TABLE_PREFIX = "cve_production_"

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger()

# HTTP session w/ retries & timeouts
session = requests.Session()
retries = Retry(
    total=3,
    backoff_factor=1,
    status_forcelist=[502,503,504],
    allowed_methods=["GET","POST"]
)
session.mount("http://", HTTPAdapter(max_retries=retries))

def fetch_and_flatten_kev(url: str, out_path: str) -> str:
    """
    Download the CISA KEV JSON, explode .vulnerabilities[] into line-delimited JSON,
    write to out_path, and return the path.
    """
    logger.info("Fetching KEV feed from %s", url)
    resp = session.get(url, timeout=(5,60))
    resp.raise_for_status()
    doc = resp.json()
    vulns = doc.get("vulnerabilities", [])
    logger.info("Flattening %d vulnerabilities to %s", len(vulns), out_path)
    with open(out_path, "w") as f:
        for v in vulns:
            f.write(json.dumps(v) + "\n")
    return out_path

def submit_druid(spec: dict):
    """Submit an ingestion spec to Druid and poll until success."""
    url = f"{DRUID_BASE}/druid/indexer/v1/task"
    resp = session.post(
        url,
        json=spec,
        headers={"Content-Type":"application/json"},
        timeout=(5, 300)
    )
    resp.raise_for_status()
    task_id = resp.json()["task"]
    logger.info("Submitted Druid task %s", task_id)
    start = time.time()
    while True:
        st = session.get(
            f"{DRUID_BASE}/druid/indexer/v1/task/{task_id}/status",
            timeout=(5, 60)
        )
        st.raise_for_status()
        status = st.json()["status"]["status"]
        if status == "SUCCESS":
            logger.info("✔ Task %s succeeded in %ds", task_id, int(time.time() - start))
            return
        if status in ("FAILED", "ABORTED"):
            logger.error("✖ Task %s ended with status %s: %s", task_id, status, st.text)
            raise RuntimeError(f"Task {task_id} failed with status {status}")
        logger.info("Task %s is %s; next check in 60s", task_id, status)
        time.sleep(60)

def bootstrap_dynamo(table_name: str):
    client = boto3.client("dynamodb", region_name=AWS_REGION)
    existing = client.list_tables()["TableNames"]
    if table_name not in existing:
        logger.info("Creating DynamoDB table %s", table_name)
        client.create_table(
            TableName=table_name,
            KeySchema=[{"AttributeName":"datasource","KeyType":"HASH"}],
            AttributeDefinitions=[{"AttributeName":"datasource","AttributeType":"S"}],
            BillingMode="PAY_PER_REQUEST"
        )
        client.get_waiter("table_exists").wait(TableName=table_name)
        logger.info("DynamoDB table %s ready", table_name)
    return boto3.resource("dynamodb", region_name=AWS_REGION).Table(table_name)

def drop_all_datasources():
    for suffix in SUFFIXES:
        ds = TABLE_PREFIX + suffix
        url = f"{DRUID_BASE}/druid/coordinator/v1/datasources/{ds}"
        logger.info("▶ Dropping Druid dataSource %s …", ds)
        resp = session.delete(url, params={"cascade": "true"}, timeout=(5, 30))
        if resp.status_code not in (200, 202, 404):
            resp.raise_for_status()
        # Poll until dropped
        for _ in range(60):
            current = session.get(
                f"{DRUID_BASE}/druid/coordinator/v1/datasources",
                timeout=(5,10)
            ).json()
            if ds not in current:
                logger.info("✔ %s dropped", ds)
                break
            time.sleep(5)
        else:
            raise RuntimeError(f"Timed out waiting for {ds} to be dropped")

def main():
    catalog    = load_catalog(name="glue", type="glue", warehouse=WAREHOUSE)
    meta_table = bootstrap_dynamo(DYNAMO_TABLE_NAME)
    s3_client  = boto3.client("s3", region_name=AWS_REGION)

    if MODE.lower() == "backfill":
        # Drop all CVE tables first
        drop_all_datasources()

    # CISA KEV Ingestion
    logger.info("▶ Ingesting CISA KEV feed into Druid")

    # Flatten to NDJSON on local disk
    kev_tmp = "/tmp/kev.ndjson"
    fetch_and_flatten_kev(
        "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json",
        kev_tmp
    )

    # Upload that NDJSON to S3 under date partition
    date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    bucket   = "cve-ingestion"
    key      = f"kev-feed/kev_{date_str}.ndjson"
    logger.info("Uploading flattened KEV to s3://%s/%s", bucket, key)
    s3_client.upload_file(kev_tmp, bucket, key)
    logger.info("Uploaded KEV NDJSON to s3://%s/%s", bucket, key)

    # Load kev ingestion spec and override to point at S3
    spec_path = os.path.join(BACKFILL_DIR, "backfill_kev_feed.json")
    with open(spec_path) as f:
        kev_spec = json.load(f)
    kev_spec["spec"]["ioConfig"]["inputSource"] = {
        "type": "s3",
        "uris": [f"s3://{bucket}/{key}"]
    }
    kev_spec["spec"]["dataSchema"]["dataSource"] = "kev_feed"
    submit_druid(kev_spec)

    # EPSS Ingestion
    logger.info("▶ Ingesting EPSS feed into Druid")

    # Download, skip the comment, add a 'date' column, write out CSV
    epss_url = "https://epss.cyentia.com/epss_scores-current.csv.gz"
    epss_gz  = "/tmp/epss.csv.gz"
    epss_csv = "/tmp/epss_with_date.csv"
    logger.info("Fetching EPSS feed from %s", epss_url)
    resp = session.get(epss_url, timeout=(5, 60))
    resp.raise_for_status()

    # Write the gz
    with open(epss_gz, "wb") as f:
        f.write(resp.content)

    logger.info("Transforming EPSS: skipping header, adding date %s", date_str)
    with gzip.open(epss_gz, "rt") as src, open(epss_csv, "w") as dst:
        for idx, line in enumerate(src):
            if idx == 0:
                # Skip the "#model_version:…" comment
                continue
            if idx == 1:
                # CSV header row: append ",date"
                dst.write(line.strip() + ",date\n")
                continue
            # Data rows: append ",{date_str}"
            dst.write(line.strip() + f",{date_str}\n")

    # Upload the transformed CSV to S3
    bucket = "cve-ingestion"
    key    = f"epss-feed/epss_{date_str}.csv"
    logger.info("Uploading EPSS to s3://%s/%s", bucket, key)
    s3_client.upload_file(epss_csv, bucket, key)
    logger.info("Uploaded EPSS CSV to s3://%s/%s", bucket, key)

    # Load & override the EPSS spec, then submit
    spec_path = os.path.join(BACKFILL_DIR, "backfill_epss_scores.json")
    with open(spec_path) as f:
        epss_spec = json.load(f)
    epss_spec["spec"]["ioConfig"]["inputSource"] = {
        "type": "s3",
        "uris": [f"s3://{bucket}/{key}"]
    }
    epss_spec["spec"]["dataSchema"]["dataSource"] = "epss_scores"
    submit_druid(epss_spec)

    # Ingesting Iceberg‐based tables
    for suffix in SUFFIXES:
        tbl_name     = TABLE_PREFIX + suffix
        full_tbl     = f"{ICEBERG_DB}.{tbl_name}"
        datasource   = tbl_name
        logger.info("▶ Processing %s (MODE=%s)", full_tbl, MODE)

        # Load table & snapshot
        table         = catalog.load_table(full_tbl)
        curr_snapshot = table.current_snapshot().snapshot_id

        # Collect manifest entries
        arrow_table   = table.inspect.entries()
        data_files    = [df["file_path"] for df in arrow_table["data_file"].to_pylist()]
        snapshot_ids  = arrow_table["snapshot_id"].to_pylist()
        statuses      = arrow_table["status"].to_pylist()

        if MODE.lower() == "backfill":
            # Full backfill
            spec_file = os.path.join(BACKFILL_DIR, f"backfill_{suffix}.json")
            with open(spec_file) as f:
                spec = json.load(f)
            input_src = spec["spec"]["ioConfig"]["inputSource"]
            input_src.pop("prefixes", None)
            input_src.pop("objects", None)
            input_src["uris"] = data_files
            spec["spec"]["dataSchema"]["dataSource"] = datasource
            submit_druid(spec)
            meta_table.put_item(
                Item={"datasource": datasource, "last_snapshot": curr_snapshot}
            )
            continue

        # incremental
        rec = meta_table.get_item(Key={"datasource": datasource}).get("Item", {})
        last_snapshot = rec.get("last_snapshot")
        if last_snapshot is None:
            raise RuntimeError(f"No watermark for {datasource}; run backfill first")

        # Pick only new files
        uris = [
            fp for fp, sid, st in zip(data_files, snapshot_ids, statuses)
            if st == ManifestEntryStatus.ADDED and sid > last_snapshot
        ]
        if not uris:
            logger.info("No new files for %s; updating watermark only", datasource)
            meta_table.update_item(
                Key={"datasource": datasource},
                UpdateExpression="SET last_snapshot = :s",
                ExpressionAttributeValues={":s": curr_snapshot},
            )
            continue

        spec_file = os.path.join(INCR_DIR, f"incremental_{suffix}.json")
        with open(spec_file) as f:
            spec = json.load(f)
        input_src = spec["spec"]["ioConfig"]["inputSource"]
        input_src.pop("prefixes", None)
        input_src.pop("objects", None)
        input_src.pop("objectGlob", None)
        input_src["uris"] = uris
        spec["spec"]["dataSchema"]["dataSource"] = datasource
        submit_druid(spec)
        meta_table.update_item(
            Key={"datasource": datasource},
            UpdateExpression="SET last_snapshot = :s",
            ExpressionAttributeValues={":s": curr_snapshot},
        )

    logger.info("All tables processed successfully")

if __name__ == "__main__":
    try:
        main()
    except Exception:
        logger.exception("Fatal error during ingestion")
        sys.exit(1)
