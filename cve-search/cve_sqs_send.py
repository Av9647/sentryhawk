#!/usr/bin/env python3
import os
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from itertools import islice
from random import uniform

import boto3
from botocore.exceptions import BotoCoreError, ClientError

# === Configuration ===
AWS_REGION  = os.getenv("AWS_REGION", "us-east-2")
QUEUE_URL   = os.getenv(
    "QUEUE_URL",
    "https://sqs.us-east-2.amazonaws.com/${ACCOUNT_ID}/cve_ingestion_vendor_product_sqs"
)
INPUT_FILE  = os.getenv("INPUT_FILE", "cve_sqs_messages_log.txt")

BATCH_SIZE  = 10          # max SQS batch size
MAX_WORKERS = 20         # number of parallel threads
MAX_RETRIES = 3
BASE_DELAY  = 0.5        # seconds

# === AWS client ===
sqs = boto3.client("sqs", region_name=AWS_REGION)


def chunked_iterable(iterable, size):
    """Yield lists of up to `size` items from `iterable`."""
    it = iter(iterable)
    while True:
        batch = list(islice(it, size))
        if not batch:
            return
        yield batch


def send_batch(entries, batch_idx):
    """
    Send one batch of up to 10 messages.
    Retries up to MAX_RETRIES on failure with exponential backoff.
    """
    payload = [{"Id": str(i), "MessageBody": body} 
               for i, body in enumerate(entries)]
    attempt = 0

    while attempt < MAX_RETRIES:
        try:
            resp = sqs.send_message_batch(
                QueueUrl=QUEUE_URL,
                Entries=payload
            )
            failed = resp.get("Failed", [])
            if failed:
                # remove any succeeded IDs from payload and retry only the rest
                failed_ids = {f["Id"] for f in failed}
                payload = [e for e in payload if e["Id"] in failed_ids]
                delay = BASE_DELAY * (2 ** attempt) + uniform(0, 0.1)
                print(f"[Batch {batch_idx}][Attempt {attempt+1}] {len(failed)} failed, retrying in {delay:.2f}s…", file=sys.stderr)
                time.sleep(delay)
                attempt += 1
                continue
            # success
            return len(entries) - len(failed)
        except (BotoCoreError, ClientError) as e:
            delay = BASE_DELAY * (2 ** attempt) + uniform(0, 0.1)
            print(f"[Batch {batch_idx}][Attempt {attempt+1}] ERROR: {e}, retrying in {delay:.2f}s…", file=sys.stderr)
            time.sleep(delay)
            attempt += 1

    # if we get here, we gave up
    print(f"[Batch {batch_idx}] FAILED after {MAX_RETRIES} attempts.", file=sys.stderr)
    return 0


def main():
    if not os.path.isfile(INPUT_FILE):
        print(f"Input file not found: {INPUT_FILE}", file=sys.stderr)
        sys.exit(1)

    # 1) Read all bodies
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        bodies = [line.rstrip("\n") for line in f if line.strip()]

    total = len(bodies)
    print(f"Loaded {total} messages into memory, batching into size={BATCH_SIZE}…")

    # 2) Make batches
    batches = list(chunked_iterable(bodies, BATCH_SIZE))
    print(f"Created {len(batches)} batches. Spawning up to {MAX_WORKERS} workers…")

    sent_count = 0
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(send_batch, batch, idx): idx
            for idx, batch in enumerate(batches, start=1)
        }
        for future in as_completed(futures):
            sent = future.result()
            sent_count += sent
            # simple progress
            done = sum(1 for f in futures if f.done())
            print(f"\rProgress: batches {done}/{len(batches)} — messages sent so far: {sent_count}", end="")

    print(f"\nAll done. Total messages successfully sent: {sent_count}/{total}")

if __name__ == "__main__":
    main()
