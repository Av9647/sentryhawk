#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Diff vendor–product combos by:
  • any change in their combined CVE list (add/remove)
  • or any CVE metadata update
since the build from at least BUFFER_HOURS ago
(or, if none that old exists, since the oldest build within the buffer interval),
then send changed combos to SQS in batches of 10.
"""
import os, re, json
from datetime import datetime, timedelta, timezone
from pymongo import MongoClient
import boto3
from itertools import islice

# ─── Config ─────────────────────────────────────────────────────────────────
MONGO_URI    = os.getenv("MONGO_URI",   "mongodb://localhost:27017/")
DB_NAME      = os.getenv("DB_NAME",     "cvedb")
QUEUE_URL    = os.getenv("DELTA_QUEUE_URL",
                 "https://sqs.us-east-2.amazonaws.com/692859941232/cve_ingestion_vendor_product_sqs")
BUFFER_HOURS = float(os.getenv("BUFFER_HOURS", "72"))
BATCH_SIZE   = 10

MAP_NVD      = "map_vendor_product_nvd"
MAP_CVELIST  = "map_vendor_product_cvelistv5"
META_COLL    = "map_build_meta"
CVE_COLL     = "cves"
# ─────────────────────────────────────────────────────────────────────────────

client = MongoClient(MONGO_URI)
db     = client[DB_NAME]
sqs    = boto3.client("sqs", region_name=os.getenv("AWS_REGION","us-east-2"))


def get_newest_ts(tag):
    doc = db[META_COLL].find_one({"collection": tag}, sort=[("ts", -1)])
    if not doc:
        raise RuntimeError(f"No builds for '{tag}'")
    return doc["ts"]  # naive


def get_prev_ts_with_buffer(tag, hours):
    docs = list(db[META_COLL]
                .find({"collection": tag})
                .sort("ts", -1))
    if len(docs) < 2:
        raise RuntimeError(f"Need ≥2 builds for '{tag}', found {len(docs)}")

    cutoff = (datetime.now(timezone.utc) - timedelta(hours=hours)).replace(tzinfo=None)
    # (1) first one ≤ cutoff
    for d in docs:
        if d["ts"] <= cutoff:
            return d["ts"]
    # (2) else oldest in window
    return docs[-1]["ts"]


# 1) pick snapshots
new_nvd_ts  = get_newest_ts("nvd_ts")
prev_nvd_ts = get_prev_ts_with_buffer("nvd_ts", BUFFER_HOURS)
new_cl_ts   = get_newest_ts("cvelistv5_ts")
prev_cl_ts  = get_prev_ts_with_buffer("cvelistv5_ts", BUFFER_HOURS)

print(f"Diffing NVD:      {prev_nvd_ts} → {new_nvd_ts}")
print(f"Diffing CVE-List: {prev_cl_ts} → {new_cl_ts}")

# 2) CVEs metadata‐changed since earlier snapshot
baseline = min(prev_nvd_ts, prev_cl_ts)
changed_cves = [c["id"] for c in db[CVE_COLL].find(
    {"$or":[
       {"lastModifiedDate": {"$gt": baseline}},
       {"publishedDate":    {"$gt": baseline}}
    ]},
    {"_id":0,"id":1})
]
print(f" → {len(changed_cves):,} CVEs metadata‐changed since {baseline.isoformat()}")

# 3) Aggregation
pipeline = [
    # 3a) new NVD docs, tag their arrays
    {"$match": {"_build_ts": new_nvd_ts}},
    {"$addFields": {"nvd_cves": "$cves", "cl_cves": []}},
    {"$project": {"cves": 0}},

    # 3b) union new CVE-List docs, tag their arrays
    {"$unionWith": {
      "coll": MAP_CVELIST,
      "pipeline": [
        {"$match": {"_build_ts": new_cl_ts}},
        {"$addFields": {"nvd_cves": [], "cl_cves": "$cves"}},
        {"$project": {"cves": 0}}
      ]
    }},

    # 3c) group to merge NVD+CL into unified new sets
    {"$group": {
      "_id": {"vendor":"$vendor","product":"$product"},
      "new_nvd_list": {"$push": "$nvd_cves"},
      "new_cl_list":  {"$push": "$cl_cves"}
    }},

    # 3d) flatten via reduce+setUnion
    {"$addFields": {
      "new_nvd": {"$reduce": {
         "input": "$new_nvd_list",
         "initialValue": [],
         "in": {"$setUnion": ["$$value","$$this"]}
      }},
      "new_cl": {"$reduce": {
         "input": "$new_cl_list",
         "initialValue": [],
         "in": {"$setUnion": ["$$value","$$this"]}
      }}
    }},

    # 3e) lookup old snapshots
    {"$lookup": {
      "from": MAP_NVD,
      "let": {"vendor":"$_id.vendor","product":"$_id.product"},
      "pipeline":[
        {"$match":{"$expr":{
          "$and":[
            {"$eq":["$vendor","$$vendor"]},
            {"$eq":["$product","$$product"]},
            {"$eq":["$_build_ts",prev_nvd_ts]}
          ]
        }}},
        {"$project":{"cves":1}}
      ],
      "as":"old_nvd_docs"
    }},
    {"$lookup": {
      "from": MAP_CVELIST,
      "let": {"vendor":"$_id.vendor","product":"$_id.product"},
      "pipeline":[
        {"$match":{"$expr":{
          "$and":[
            {"$eq":["$vendor","$$vendor"]},
            {"$eq":["$product","$$product"]},
            {"$eq":["$_build_ts",prev_cl_ts]}
          ]
        }}},
        {"$project":{"cves":1}}
      ],
      "as":"old_cl_docs"
    }},

    # 3f) extract old arrays
    {"$addFields": {
      "old_nvd": {"$ifNull":[{"$arrayElemAt":["$old_nvd_docs.cves",0]},[]]},
      "old_cl":  {"$ifNull":[{"$arrayElemAt":["$old_cl_docs.cves",0]},[]]}
    }},

    # 3g) true two‐way diff
    {"$match": {"$expr": {"$or":[
      # NVD additions
      {"$gt":[{"$size":{"$setDifference":["$new_nvd","$old_nvd"]}},0]},
      # NVD removals
      {"$gt":[{"$size":{"$setDifference":["$old_nvd","$new_nvd"]}},0]},
      # CL additions
      {"$gt":[{"$size":{"$setDifference":["$new_cl","$old_cl"]}},0]},
      # CL removals
      {"$gt":[{"$size":{"$setDifference":["$old_cl","$new_cl"]}},0]}
    ]}}},

    # 3h) bring in metadata‐changed combos
    {"$unionWith": {
      "coll": MAP_NVD,
      "pipeline":[
        {"$match":{"cves":{"$in": changed_cves}}},
        {"$project":{"vendor":1,"product":1}}
      ]
    }},
    {"$unionWith": {
      "coll": MAP_CVELIST,
      "pipeline":[
        {"$match":{"cves":{"$in": changed_cves}}},
        {"$project":{"vendor":1,"product":1}}
      ]
    }},

    # 3i) dedupe and re‐emit
    {"$group": {"_id":"$_id"}},
    {"$project": {"_id":0, "vendor":"$_id.vendor","product":"$_id.product"}}
]

def batched(it,n):
    it = iter(it)
    while True:
        batch = list(islice(it,n))
        if not batch:
            return
        yield batch

# 4) Run & SQS
cursor = db[MAP_NVD].aggregate(pipeline, allowDiskUse=True)
run_ts = datetime.now(timezone.utc).isoformat()
sent   = 0

for batch in batched(cursor, BATCH_SIZE):
    entries = []
    for d in batch:
        raw_id  = f"{d['vendor']}_{d['product']}"
        safe_id = re.sub(r'[^A-Za-z0-9_-]','_',raw_id)[:80]
        entries.append({
          "Id": safe_id,
          "MessageBody": json.dumps({
            "vendor": d['vendor'],
            "product": d['product'],
            "ingestionTimestamp": run_ts
          }, ensure_ascii=False)
        })
    resp = sqs.send_message_batch(QueueUrl=QUEUE_URL, Entries=entries)
    if resp.get("FailedPutCount",0):
        failed = [f["Id"] for f in resp["Failed"]]
        raise RuntimeError(f"SQS batch send failed for IDs: {failed}")
    sent += len(entries)

print(f"✔ Diff complete; sent {sent} messages to SQS.")
