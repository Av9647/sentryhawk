#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Diff vendor–product combos by:
  • any change in their combined CVE list (add/remove)
  • or any CVE metadata update
since the build from at least BUFFER_HOURS ago
(or, if none that old exists, since the oldest build within the buffer interval),
then send changed combos to SQS in batches of 10.
"""
import os, re, json, hashlib
from datetime import datetime, timedelta, timezone
from pymongo import MongoClient
import boto3
from itertools import islice

# ─── Config ─────────────────────────────────────────────────────────────────
MONGO_URI    = os.getenv("MONGO_URI",   "mongodb://localhost:27017/")
DB_NAME      = os.getenv("DB_NAME",     "cvedb")
QUEUE_URL    = os.getenv("DELTA_QUEUE_URL",
                 "https://sqs.us-east-2.amazonaws.com/${ACCOUNT_ID}/cve_ingestion_vendor_product_sqs")
BUFFER_HOURS = float(os.getenv("BUFFER_HOURS", "72"))
BATCH_SIZE   = 10

MAP_NVD      = "map_vendor_product_nvd"
MAP_CVELIST  = "map_vendor_product_cvelistv5"
META_COLL    = "map_build_meta"
CVE_COLL     = "cves"
AWS_REGION   = os.getenv("AWS_REGION","us-east-2")
# ─────────────────────────────────────────────────────────────────────────────

client = MongoClient(MONGO_URI)
db     = client[DB_NAME]
sqs    = boto3.client("sqs", region_name=AWS_REGION)


def get_newest_ts(tag):
    doc = db[META_COLL].find_one({"collection": tag}, sort=[("ts", -1)])
    if not doc:
        raise RuntimeError(f"No builds for '{tag}'")
    return doc["ts"]  # naive


def get_prev_ts_with_buffer(tag, hours):
    docs = list(db[META_COLL]
                .find({"collection": tag})
                .sort("ts", -1))
    if len(docs) < 2:
        raise RuntimeError(f"Need ≥2 builds for '{tag}', found {len(docs)}")

    cutoff = (datetime.now(timezone.utc) - timedelta(hours=hours)).replace(tzinfo=None)
    # (1) first one ≤ cutoff
    for d in docs:
        if d["ts"] <= cutoff:
            return d["ts"]
    # (2) else oldest in window
    return docs[-1]["ts"]


# 1) pick snapshots
new_nvd_ts  = get_newest_ts("nvd_ts")
prev_nvd_ts = get_prev_ts_with_buffer("nvd_ts", BUFFER_HOURS)
new_cl_ts   = get_newest_ts("cvelistv5_ts")
prev_cl_ts  = get_prev_ts_with_buffer("cvelistv5_ts", BUFFER_HOURS)

print(f"Diffing NVD:      {prev_nvd_ts} → {new_nvd_ts}")
print(f"Diffing CVE-List: {prev_cl_ts} → {new_cl_ts}")

# 2) CVEs metadata‐changed since earlier snapshot
baseline = min(prev_nvd_ts, prev_cl_ts)
changed_cves = [c["id"] for c in db[CVE_COLL].find(
    {"$or":[
       {"lastModifiedDate": {"$gt": baseline}},
       {"publishedDate":    {"$gt": baseline}}
    ]},
    {"_id":0,"id":1})
]
print(f" → {len(changed_cves):,} CVEs metadata‐changed since {baseline.isoformat()}")

# 3) Aggregation
pipeline = [
    # 3a) new NVD docs, tag their arrays
    {"$match": {"_build_ts": new_nvd_ts}},
    {"$addFields": {"nvd_cves": "$cves", "cl_cves": []}},
    {"$project": {"cves": 0, "_id": 0}},  # shape: vendor, product, nvd_cves, cl_cves

    # 3b) union new CVE-List docs, tag their arrays
    {"$unionWith": {
      "coll": MAP_CVELIST,
      "pipeline": [
        {"$match": {"_build_ts": new_cl_ts}},
        {"$addFields": {"nvd_cves": [], "cl_cves": "$cves"}},
        {"$project": {"cves": 0, "_id": 0}}
      ]
    }},

    # 3c) group to merge NVD+CL into unified new sets
    {"$group": {
      "_id": {"vendor":"$vendor","product":"$product"},
      "new_nvd_list": {"$push": "$nvd_cves"},
      "new_cl_list":  {"$push": "$cl_cves"}
    }},

    # 3d) flatten via reduce+setUnion
    {"$addFields": {
      "new_nvd": {"$reduce": {
         "input": "$new_nvd_list",
         "initialValue": [],
         "in": {"$setUnion": ["$$value","$$this"]}
      }},
      "new_cl": {"$reduce": {
         "input": "$new_cl_list",
         "initialValue": [],
         "in": {"$setUnion": ["$$value","$$this"]}
      }}
    }},

    # 3e) lookup old snapshots
    {"$lookup": {
      "from": MAP_NVD,
      "let": {"vendor":"$_id.vendor","product":"$_id.product"},
      "pipeline":[
        {"$match":{"$expr":{
          "$and":[
            {"$eq":["$vendor","$$vendor"]},
            {"$eq":["$product","$$product"]},
            {"$eq":["$_build_ts",prev_nvd_ts]}
          ]
        }}},
        {"$project":{"cves":1, "_id":0}}
      ],
      "as":"old_nvd_docs"
    }},
    {"$lookup": {
      "from": MAP_CVELIST,
      "let": {"vendor":"$_id.vendor","product":"$_id.product"},
      "pipeline":[
        {"$match":{"$expr":{
          "$and":[
            {"$eq":["$vendor","$$vendor"]},
            {"$eq":["$product","$$product"]},
            {"$eq":["$_build_ts",prev_cl_ts]}
          ]
        }}},
        {"$project":{"cves":1, "_id":0}}
      ],
      "as":"old_cl_docs"
    }},

    # 3f) extract old arrays
    {"$addFields": {
      "old_nvd": {"$ifNull":[{"$arrayElemAt":["$old_nvd_docs.cves",0]},[]]},
      "old_cl":  {"$ifNull":[{"$arrayElemAt":["$old_cl_docs.cves",0]},[]]}
    }},

    # 3g) true two‐way diff
    {"$match": {"$expr": {"$or":[
      {"$gt":[{"$size":{"$setDifference":["$new_nvd","$old_nvd"]}},0]},  # NVD additions
      {"$gt":[{"$size":{"$setDifference":["$old_nvd","$new_nvd"]}},0]},  # NVD removals
      {"$gt":[{"$size":{"$setDifference":["$new_cl","$old_cl"]}},0]},    # CL additions
      {"$gt":[{"$size":{"$setDifference":["$old_cl","$new_cl"]}},0]}     # CL removals
    ]}}},

    # 3h) bring in metadata‐changed combos (from both maps)
    {"$unionWith": {
      "coll": MAP_NVD,
      "pipeline":[
        {"$match":{"cves":{"$in": changed_cves}}},
        {"$project":{"vendor":1,"product":1,"_id":0}}
      ]
    }},
    {"$unionWith": {
      "coll": MAP_CVELIST,
      "pipeline":[
        {"$match":{"cves":{"$in": changed_cves}}},
        {"$project":{"vendor":1,"product":1,"_id":0}}
      ]
    }},

    # 3i) normalize shape then dedupe by (vendor, product)
    {"$addFields": {
      "vendor":  {"$ifNull": ["$vendor",  "$_id.vendor"]},
      "product": {"$ifNull": ["$product", "$_id.product"]}
    }},
    {"$group": {"_id": {"vendor":"$vendor","product":"$product"}}},
    {"$project": {"_id":0, "vendor":"$_id.vendor","product":"$_id.product"}}
]

def batched(it,n):
    it = iter(it)
    while True:
        batch = list(islice(it,n))
        if not batch:
            return
        yield batch

def make_sqs_id(vendor: str, product: str) -> str:
    """
    Build a collision-resistant, readable SQS batch Id (≤80 chars).
    Does NOT alter the vendor/product values—only the transport Id.
    """
    raw_id  = f"{vendor}_{product}"
    safe_id = re.sub(r'[^A-Za-z0-9_-]', '_', raw_id)

    # 12-hex hash suffix (BLAKE2b 6-byte digest → 12 hex chars)
    suffix = hashlib.blake2b(raw_id.encode("utf-8"), digest_size=6).hexdigest()
    # Reserve 1 for '-' + 12 for suffix
    base   = safe_id[: (80 - 1 - 12)] if len(safe_id) > (80 - 1 - 12) else safe_id
    return f"{base}-{suffix}"

# 4) Run & SQS
cursor = db[MAP_NVD].aggregate(pipeline, allowDiskUse=True)
run_ts = datetime.now(timezone.utc).isoformat()
sent   = 0

for batch in batched(cursor, BATCH_SIZE):
    entries = []
    seen_ids = set()
    for d in batch:
        ven = d["vendor"]
        prod = d["product"]

        uniq_id = make_sqs_id(ven, prod)
        # ensure uniqueness within this batch
        if uniq_id in seen_ids:
            # append a tiny counter if (extremely unlikely) collision in same batch
            i = 1
            candidate = f"{uniq_id}-x{i}"
            while candidate in seen_ids and len(candidate) <= 80:
                i += 1
                candidate = f"{uniq_id}-x{i}"
            uniq_id = candidate[:80]
        seen_ids.add(uniq_id)

        entries.append({
          "Id": uniq_id,
          "MessageBody": json.dumps({
            "vendor": ven,
            "product": prod,
            "ingestionTimestamp": run_ts
          }, ensure_ascii=False)
        })

    resp = sqs.send_message_batch(QueueUrl=QUEUE_URL, Entries=entries)

    failed = resp.get("Failed", [])
    if failed:
        raise RuntimeError(f"SQS batch send failed for IDs: {[f.get('Id') for f in failed]}")

    sent += len(entries)

print(f"✔ Diff complete; sent {sent} messages to SQS.")
