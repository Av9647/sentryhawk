#!/usr/bin/env python3
import os
import sqlite3
import threading
import queue
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor

import boto3

# === AWS clients ===
AWS_REGION = os.getenv("AWS_REGION", "us-east-2")
sqs = boto3.client("sqs", region_name=AWS_REGION)
s3  = boto3.client("s3")

# === Parameters ===
QUEUE_URL   = os.getenv("QUEUE_URL",
    "https://sqs.us-east-2.amazonaws.com/692859941232/cve_ingestion_vendor_product_sqs_dlq"
)
OUTPUT_FILE = os.getenv("OUTPUT_FILE", "cve_sqs_messages_log.txt")
S3_BUCKET   = os.getenv("S3_BUCKET", "cve-code")
S3_PREFIX   = os.getenv("S3_PREFIX", "sqs_logs")
TIMESTAMP   = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
S3_KEY      = f"{S3_PREFIX}/cve_sqs_log_{TIMESTAMP}.txt"

# === SQLite setup (WAL mode for speed) ===
DB_PATH = os.getenv("DB_PATH", "processed_messages.db")
db = sqlite3.connect(DB_PATH, check_same_thread=False)
db.execute("PRAGMA journal_mode = WAL;")
db.execute("PRAGMA synchronous = NORMAL;")
cursor = db.cursor()
cursor.execute("""
    CREATE TABLE IF NOT EXISTS processed (
        message_id TEXT PRIMARY KEY
    )
""")
db.commit()

# === Thread-safe queue and shutdown flag ===
msg_queue     = queue.Queue(maxsize=10_000)
shutdown_flag = threading.Event()

def poll_worker():
    """Continuously long-poll SQS; push every received message into msg_queue."""
    empty_count = 0
    while empty_count < 3 and not shutdown_flag.is_set():
        resp = sqs.receive_message(
            QueueUrl=QUEUE_URL,
            MaxNumberOfMessages=10,
            WaitTimeSeconds=10,      # long polling
            VisibilityTimeout=30     # give us more breathing room
        )
        msgs = resp.get("Messages", [])
        if not msgs:
            empty_count += 1
            continue
        empty_count = 0
        for m in msgs:
            msg_queue.put(m)
    # when this worker exits, it simply stops polling

def writer_worker():
    """Consume from msg_queue, dedupe via SQLite, write body to file, delete from SQS."""
    commit_batch = 100
    processed_since_commit = 0

    with open(OUTPUT_FILE, "a", encoding="utf-8", buffering=1) as f:
        while not (shutdown_flag.is_set() and msg_queue.empty()):
            try:
                msg = msg_queue.get(timeout=1)
            except queue.Empty:
                continue

            msg_id = msg["MessageId"]
            # skip dupes
            cursor.execute(
                "SELECT 1 FROM processed WHERE message_id = ?",
                (msg_id,)
            )
            if cursor.fetchone() is None:
                # — write to disk
                f.write(msg["Body"] + "\n")
                f.flush()
                os.fsync(f.fileno())

                # — record in SQLite
                cursor.execute(
                    "INSERT INTO processed(message_id) VALUES(?)",
                    (msg_id,)
                )
                processed_since_commit += 1

                # — delete from SQS (with a tiny retry)
                for attempt in range(2):
                    try:
                        sqs.delete_message(
                            QueueUrl=QUEUE_URL,
                            ReceiptHandle=msg["ReceiptHandle"]
                        )
                        break
                    except Exception as e:
                        print(f"Warning: delete_message failed (attempt {attempt+1}): {e}")

                # — batch commit
                if processed_since_commit >= commit_batch:
                    db.commit()
                    processed_since_commit = 0

            # mark done (even for duplicates)
            msg_queue.task_done()

        # commit any remaining
        if processed_since_commit:
            db.commit()

def main():
    # start writer
    writer = threading.Thread(target=writer_worker, daemon=True)
    writer.start()

    # start multiple pollers
    num_workers = 5
    with ThreadPoolExecutor(max_workers=num_workers) as ex:
        futures = [ex.submit(poll_worker) for _ in range(num_workers)]
        for f in futures:
            f.result()

    # signal writer to finish once queue is drained
    shutdown_flag.set()
    writer.join()

    # upload log
    print(f"Uploading {OUTPUT_FILE} → s3://{S3_BUCKET}/{S3_KEY}")
    s3.upload_file(OUTPUT_FILE, S3_BUCKET, S3_KEY)

    db.close()
    print("Done. Processed DB retained at", DB_PATH)

if __name__ == "__main__":
    main()
